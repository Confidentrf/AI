{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyFQMSd73KlaHZgXqzPrGg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Confidentrf/AI/blob/Machine-Learning/NaiveBayes_email_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Naive Bayes**"
      ],
      "metadata": {
        "id": "KaOsmrR1KsVL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zrukK1liKqhO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Reads emails in a file and loops through them\n",
        "def readFiles(path):\n",
        "    for root, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            path = os.path.join(root, filename)\n",
        "\n",
        "            inBody = False\n",
        "            lines = []\n",
        "            f = io.open(path, 'r', encoding='latin1')\n",
        "            for line in f:\n",
        "                if inBody:\n",
        "                    lines.append(line)\n",
        "                elif line == '\\n':\n",
        "                    inBody = True\n",
        "            f.close()\n",
        "            message = '\\n'.join(lines)\n",
        "            yield path, message\n",
        "\n",
        "#Creats Dataframe object from files.\n",
        "def dataFrameFromDirectory(path, classification):\n",
        "    rows = []\n",
        "    index = []\n",
        "    for filename, message in readFiles(path):\n",
        "        rows.append({'message': message, 'class': classification})\n",
        "        index.append(filename)\n",
        "\n",
        "    return DataFrame(rows, index=index)\n",
        "\n",
        "data = DataFrame({'message': [], 'class': []})\n",
        "\n",
        "data = pd.concat([data, dataFrameFromDirectory('emails/spam', 'spam')])\n",
        "data = pd.concat([data, dataFrameFromDirectory('emails/ham', 'ham')])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at that DataFrame:"
      ],
      "metadata": {
        "id": "n3kYsi6rN1K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "g5igZhwAN3fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use a CountVectorizer to split up each message into its list of words, and throw that into a MultinomialNB classifier. Call fit() and we've got a trained spam filter ready to go! It's just that easy."
      ],
      "metadata": {
        "id": "sSZL1vqsN4uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "counts = vectorizer.fit_transform(data['message'].values)\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "targets = data['class'].values\n",
        "classifier.fit(counts, targets)"
      ],
      "metadata": {
        "id": "aKUm4rKhN7Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See it in action"
      ],
      "metadata": {
        "id": "XytT_zU1N9zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\n",
        "example_counts = vectorizer.transform(examples)\n",
        "predictions = classifier.predict(example_counts)\n",
        "predictions"
      ],
      "metadata": {
        "id": "OqdbGG4sOAOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vehicle speed classifications (slow or fast)"
      ],
      "metadata": {
        "id": "dQc8RszrOEE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "n_samples = 10000\n",
        "\n",
        "# To generate our fabricated data set, we'll create two blobs of\n",
        "# randomly distributed data at two corners of the graph...\n",
        "centers = [(0.9, 0.05), (0.05, 0.94)]\n",
        "X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=0.4,\n",
        "                  centers=centers, shuffle=False, random_state=42)\n",
        "\n",
        "# Then filter out only the values that lie within [0,1]:\n",
        "features = []\n",
        "labels = []\n",
        "for feature, label in zip(X, y):\n",
        "    if (feature[0] >= 0 and feature[0] <= 1.0 and feature[1] >= 0 and feature[1] <= 1.0):\n",
        "        features.append(feature)\n",
        "        labels.append(label)\n",
        "\n",
        "# And convert the results back into numpy arrays:\n",
        "X = np.array(features)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "dMaqsm9MONFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}